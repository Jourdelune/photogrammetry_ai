{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c5f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extraction des descripteurs ORB...\n",
      "üîó Recherche des images connect√©es...\n",
      "\n",
      "‚úÖ S√©quence d‚Äôimages connect√©es trouv√©e :\n",
      "  - /home/jourdelune/Images/colmap/input/image1.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image10.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image12.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image13.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image14.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image16.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image11.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image17.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image15.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image20.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image21.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image3.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image2.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image18.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image24.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image27.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image4.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image22.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image23.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image25.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image26.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image5.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image6.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image7.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image8.jpg\n",
      "  - /home/jourdelune/Images/colmap/input/image9.jpg\n",
      "le nombre d'images manquantes est : 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "def compute_feature_matches(image_paths, min_matches=20):\n",
    "    orb = cv2.ORB_create(nfeatures=1000)\n",
    "    index_params = dict(algorithm=6, table_number=6, key_size=12, multi_probe_level=1)\n",
    "    search_params = {}\n",
    "    matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    keypoints_descriptors = {}\n",
    "    matches_graph = defaultdict(list)\n",
    "\n",
    "    print(\"üîç Extraction des descripteurs ORB...\")\n",
    "\n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "        if descriptors is not None:\n",
    "            keypoints_descriptors[path] = descriptors\n",
    "\n",
    "    print(\"üîó Recherche des images connect√©es...\")\n",
    "\n",
    "    for i, path1 in enumerate(image_paths):\n",
    "        desc1 = keypoints_descriptors.get(path1)\n",
    "        if desc1 is None:\n",
    "            continue\n",
    "\n",
    "        for j, path2 in enumerate(image_paths):\n",
    "            if i == j:\n",
    "                continue\n",
    "            desc2 = keypoints_descriptors.get(path2)\n",
    "            if desc2 is None:\n",
    "                continue\n",
    "\n",
    "            matches = matcher.knnMatch(desc1, desc2, k=2)\n",
    "            good = []\n",
    "            for m_n in matches:\n",
    "                if len(m_n) < 2:\n",
    "                    continue\n",
    "                m, n = m_n\n",
    "                if m.distance < 0.75 * n.distance:\n",
    "                    good.append(m)\n",
    "\n",
    "            if len(good) >= min_matches:\n",
    "                matches_graph[path1].append(path2)\n",
    "\n",
    "    return matches_graph\n",
    "\n",
    "\n",
    "def extract_image_sequence(matches_graph):\n",
    "    visited = set()\n",
    "    sequence = []\n",
    "\n",
    "    def dfs(node):\n",
    "        visited.add(node)\n",
    "        sequence.append(node)\n",
    "        for neighbor in matches_graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                return dfs(neighbor)\n",
    "\n",
    "    # Find a node with outgoing edges\n",
    "    for start_node in matches_graph:\n",
    "        if start_node not in visited:\n",
    "            dfs(start_node)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "# === MAIN ===\n",
    "\n",
    "image_dir = \"/home/jourdelune/Images/colmap/input\"\n",
    "image_paths = sorted(\n",
    "    [\n",
    "        os.path.join(image_dir, fname)\n",
    "        for fname in os.listdir(image_dir)\n",
    "        if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "graph = compute_feature_matches(image_paths, min_matches=30)\n",
    "sequence = extract_image_sequence(graph)\n",
    "\n",
    "print(\"\\n‚úÖ S√©quence d‚Äôimages connect√©es trouv√©e :\")\n",
    "for img in sequence:\n",
    "    print(\"  -\", img)\n",
    "\n",
    "# missing image\n",
    "print(\"le nombre d'images manquantes est :\", len(image_paths) - len(sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2afa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jourdelune/Bureau/dev/photogrammetry_ai/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 26, Batch size: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_394095/1283861933.py:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=dtype):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from torch.nn import functional as F\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "from vggt.utils.helper import create_pixel_coordinate_grid, randomly_limit_trues\n",
    "from vggt.utils.load_fn import load_and_preprocess_images_square\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "model = VGGT.from_pretrained(\"facebook/VGGT-1B\").to(device)\n",
    "\n",
    "image_dir = \"/home/jourdelune/Images/colmap/input\"\n",
    "image_names = sequence\n",
    "\n",
    "vggt_fixed_resolution = 518\n",
    "img_load_resolution = 1024\n",
    "batch_size = 3  # max images per VGGT run\n",
    "\n",
    "# Load all images\n",
    "images_all, original_coords_all = load_and_preprocess_images_square(\n",
    "    image_names, img_load_resolution\n",
    ")\n",
    "\n",
    "# Split into batches\n",
    "total_images = images_all.shape[0]\n",
    "batched_extrinsic, batched_intrinsic = [], []\n",
    "batched_points_3d, batched_points_rgb, batched_points_xyf = [], [], []\n",
    "\n",
    "print(f\"Total images: {total_images}, Batch size: {batch_size}\")\n",
    "\n",
    "for i in range(0, total_images, batch_size):\n",
    "    images_batch = images_all[i : i + batch_size].to(device)\n",
    "    original_coords = original_coords_all[i : i + batch_size].to(device)\n",
    "\n",
    "    # Resize and run VGGT\n",
    "    images_resized = F.interpolate(\n",
    "        images_batch,\n",
    "        size=(vggt_fixed_resolution, vggt_fixed_resolution),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "            images_input = images_resized[None]\n",
    "            aggregated_tokens_list, ps_idx = model.aggregator(images_input)\n",
    "            pose_enc = model.camera_head(aggregated_tokens_list)[-1]\n",
    "            extrinsic, intrinsic = pose_encoding_to_extri_intri(\n",
    "                pose_enc, images_input.shape[-2:]\n",
    "            )\n",
    "            depth_map, depth_conf = model.depth_head(\n",
    "                aggregated_tokens_list, images_input, ps_idx\n",
    "            )\n",
    "\n",
    "    extrinsic = extrinsic.squeeze(0).cpu().numpy()\n",
    "    intrinsic = intrinsic.squeeze(0).cpu().numpy()\n",
    "    depth_map = depth_map.squeeze(0).cpu().numpy()\n",
    "    depth_conf = depth_conf.squeeze(0).cpu().numpy()\n",
    "\n",
    "    points_3d = unproject_depth_map_to_point_map(depth_map, extrinsic, intrinsic)\n",
    "\n",
    "    image_size = np.array([vggt_fixed_resolution, vggt_fixed_resolution])\n",
    "    num_frames, height, width, _ = points_3d.shape\n",
    "\n",
    "    points_rgb = (images_resized.cpu().numpy() * 255).astype(np.uint8)\n",
    "    points_rgb = points_rgb.transpose(0, 2, 3, 1)\n",
    "    points_xyf = create_pixel_coordinate_grid(num_frames, height, width)\n",
    "\n",
    "    conf_thres_value = 5.0\n",
    "    max_points_for_colmap = 100000\n",
    "    conf_mask = depth_conf >= conf_thres_value\n",
    "    conf_mask = randomly_limit_trues(conf_mask, max_points_for_colmap)\n",
    "\n",
    "    batched_extrinsic.append(extrinsic)\n",
    "    batched_intrinsic.append(intrinsic)\n",
    "    batched_points_3d.append(points_3d[conf_mask])\n",
    "    batched_points_rgb.append(points_rgb[conf_mask])\n",
    "    batched_points_xyf.append(points_xyf[conf_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5fab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "def draw_registration_result(source, target, transformation = np.identity(4)):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp],\n",
    "                                      zoom=1,\n",
    "                                      front=[0.9288, -0.2951, -0.2242],\n",
    "                                      lookat=[1.6784, 2.0612, 1.4451],\n",
    "                                      up=[-0.3402, -0.9189, -0.1996])\n",
    "    \n",
    "\n",
    "def align_point_clouds(source: Union[o3d.geometry.PointCloud, np.ndarray],\n",
    "                  target: Union[o3d.geometry.PointCloud, np.ndarray],\n",
    "                  threshold: float = 0.4,\n",
    "                  max_iteration: int = 400) -> o3d.pipelines.registration.RegistrationResult:\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, threshold, np.identity(4),\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=max_iteration))\n",
    "    return reg_p2p\n",
    "\n",
    "def merge_and_draw(source: o3d.geometry.PointCloud, target: o3d.geometry.PointCloud, transformation: np.ndarray = np.identity(4)) -> o3d.geometry.PointCloud:\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.transform(transformation)\n",
    "    merged = source_temp + target_temp\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f64eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate source from the first batch\n",
    "source = o3d.geometry.PointCloud()\n",
    "source.points = o3d.utility.Vector3dVector(batched_points_3d[0])\n",
    "source.colors = o3d.utility.Vector3dVector(batched_points_rgb[0] / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e39814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target from the second batch\n",
    "target = o3d.geometry.PointCloud()\n",
    "target.points = o3d.utility.Vector3dVector(batched_points_3d[1])\n",
    "target.colors = o3d.utility.Vector3dVector(batched_points_rgb[1] / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa31116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "source_vis = copy.deepcopy(source)\n",
    "target_vis = copy.deepcopy(target)\n",
    "\n",
    "# G√©n√®re une transformation de translation (d√©calage)\n",
    "translation = np.identity(4)\n",
    "translation[:3, 3] = [2, 0.2, -0.3]  # exemple de d√©calage (x, y, z)\n",
    "\n",
    "target_vis.transform(translation)\n",
    "\n",
    "o3d.visualization.draw_geometries([source_vis, target_vis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d4f2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply point-to-point ICP\n",
    "reg_p2p = align_point_clouds(source, target, max_iteration=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "392a94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([merge_and_draw(source, target, reg_p2p.transformation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbf660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photogrammetry-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
