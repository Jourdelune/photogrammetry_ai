{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['XDG_SESSION_TYPE'] = 'x11'\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from photogrammetry_ai import (\n",
    "    PhotogrammetryPipeline,\n",
    "    LightGlueMatcher,\n",
    "    VGGTReconstructor,\n",
    "    ICPAligner,\n",
    ")\n",
    "import os\n",
    "\n",
    "image_dir = \"/home/jourdelune/Images/colmap/input\"\n",
    "\n",
    "images = os.listdir(image_dir)\n",
    "images = [\n",
    "    os.path.join(image_dir, img)\n",
    "    for img in images\n",
    "    if img.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "]\n",
    "\n",
    "\n",
    "pipeline = PhotogrammetryPipeline(\n",
    "    matcher=LightGlueMatcher(),  # used to create related batches of images\n",
    "    reconstructor=VGGTReconstructor(),  # used to reconstruct the 3D points from the images\n",
    "    aligner=ICPAligner(),  # used to merges the 3D points from the batches\n",
    "    max_batch_size=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880049e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches, missing_images = pipeline.build_batches(images, display_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d796ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "for batch in batches:\n",
    "    sequence.extend(batch)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2afa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from torch.nn import functional as F\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "from vggt.utils.helper import create_pixel_coordinate_grid, randomly_limit_trues\n",
    "from vggt.utils.load_fn import load_and_preprocess_images_square\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "model = VGGT.from_pretrained(\"facebook/VGGT-1B\").to(device)\n",
    "\n",
    "image_dir = \"/home/jourdelune/Images/colmap/input\"\n",
    "image_names = sequence\n",
    "\n",
    "vggt_fixed_resolution = 518\n",
    "img_load_resolution = 1024\n",
    "batch_size = 3  # max images per VGGT run\n",
    "\n",
    "# Load all images\n",
    "images_all, original_coords_all = load_and_preprocess_images_square(\n",
    "    image_names, img_load_resolution\n",
    ")\n",
    "\n",
    "# Split into batches\n",
    "total_images = images_all.shape[0]\n",
    "batched_extrinsic, batched_intrinsic = [], []\n",
    "batched_points_3d, batched_points_rgb, batched_points_xyf = [], [], []\n",
    "\n",
    "print(f\"Total images: {total_images}, Batch size: {batch_size}\")\n",
    "\n",
    "for i in range(0, total_images, batch_size):\n",
    "    images_batch = images_all[i : i + batch_size].to(device)\n",
    "    original_coords = original_coords_all[i : i + batch_size].to(device)\n",
    "\n",
    "    # Resize and run VGGT\n",
    "    images_resized = F.interpolate(\n",
    "        images_batch,\n",
    "        size=(vggt_fixed_resolution, vggt_fixed_resolution),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "            images_input = images_resized[None]\n",
    "            aggregated_tokens_list, ps_idx = model.aggregator(images_input)\n",
    "            pose_enc = model.camera_head(aggregated_tokens_list)[-1]\n",
    "            extrinsic, intrinsic = pose_encoding_to_extri_intri(\n",
    "                pose_enc, images_input.shape[-2:]\n",
    "            )\n",
    "            depth_map, depth_conf = model.depth_head(\n",
    "                aggregated_tokens_list, images_input, ps_idx\n",
    "            )\n",
    "\n",
    "    extrinsic = extrinsic.squeeze(0).cpu().numpy()\n",
    "    intrinsic = intrinsic.squeeze(0).cpu().numpy()\n",
    "    depth_map = depth_map.squeeze(0).cpu().numpy()\n",
    "    depth_conf = depth_conf.squeeze(0).cpu().numpy()\n",
    "\n",
    "    points_3d = unproject_depth_map_to_point_map(depth_map, extrinsic, intrinsic)\n",
    "\n",
    "    image_size = np.array([vggt_fixed_resolution, vggt_fixed_resolution])\n",
    "    num_frames, height, width, _ = points_3d.shape\n",
    "\n",
    "    points_rgb = (images_resized.cpu().numpy() * 255).astype(np.uint8)\n",
    "    points_rgb = points_rgb.transpose(0, 2, 3, 1)\n",
    "    points_xyf = create_pixel_coordinate_grid(num_frames, height, width)\n",
    "\n",
    "    conf_thres_value = 5.0\n",
    "    max_points_for_colmap = 100000\n",
    "    conf_mask = depth_conf >= conf_thres_value\n",
    "    conf_mask = randomly_limit_trues(conf_mask, max_points_for_colmap)\n",
    "\n",
    "    batched_extrinsic.append(extrinsic)\n",
    "    batched_intrinsic.append(intrinsic)\n",
    "    batched_points_3d.append(points_3d[conf_mask])\n",
    "    batched_points_rgb.append(points_rgb[conf_mask])\n",
    "    batched_points_xyf.append(points_xyf[conf_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def merge_and_draw(source: o3d.geometry.PointCloud, target: o3d.geometry.PointCloud, transformation: np.ndarray = np.identity(4)) -> o3d.geometry.PointCloud:\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.transform(transformation)\n",
    "    merged = source_temp + target_temp\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42f64eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate source from the first batch\n",
    "source = o3d.geometry.PointCloud()\n",
    "source.points = o3d.utility.Vector3dVector(batched_points_3d[0])\n",
    "source.colors = o3d.utility.Vector3dVector(batched_points_rgb[0] / 255.0)\n",
    "source.estimate_normals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76e39814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target from the second batch\n",
    "target = o3d.geometry.PointCloud()\n",
    "target.points = o3d.utility.Vector3dVector(batched_points_3d[1])\n",
    "target.colors = o3d.utility.Vector3dVector(batched_points_rgb[1] / 255.0)\n",
    "target.estimate_normals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfa31116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "source_vis = copy.deepcopy(source)\n",
    "target_vis = copy.deepcopy(target)\n",
    "\n",
    "# Génère une transformation de translation (décalage)\n",
    "translation = np.identity(4)\n",
    "# translation[:3, 3] = [2, 0.2, -0.3]  # exemple de décalage (x, y, z)\n",
    "\n",
    "target_vis.transform(translation)\n",
    "\n",
    "o3d.visualization.draw_geometries([source_vis, target_vis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46cea5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 \n",
    "threshold = 1.0\n",
    "loss = o3d.pipelines.registration.TukeyLoss(k=sigma)\n",
    "p2l = o3d.pipelines.registration.TransformationEstimationPointToPlane(loss)\n",
    "result_icp = o3d.pipelines.registration.registration_icp(source, target,\n",
    "                                                      threshold, translation,\n",
    "                                                      p2l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "392a94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([merge_and_draw(source, target, result_icp.transformation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbf660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photogrammetry-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
